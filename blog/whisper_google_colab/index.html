<!DOCTYPE html>
<html lang="de-AT"><head>
  
  

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="favicon.png">
  <script async defer data-website-id="030ec4d1-80d8-4c4a-b767-1a9ff8c68ece" src="https://kaleidoscopic-faun-b8a266.netlify.app/protocol.js"></script>
  
  <title>
    Kostenloses Speech-To-Text AI Tool für Transkribieren mit Google Colab - Fatos Halimi
  </title>
  <meta name="description" content="Whisper ist ein System zur automatischen Spracherkennung ASR) von OpenAI, dass in 680000 Stunden an überwachten Daten aus dem Internet trainiert und kostenlos in Github zur Verfügung gestellt.
Warum ist das so interessant? Nun hat man eine kostenlose Möglichkeit, Sprachaufnahmen direkt transkribieren und in andere Sprachen übersetzen zu lassen.
In dem Post schauen wir uns mal an, wie man beispielsweise die Aufnahme eines Interviews einfach und kostenlos transkribieren kann. Dazu verwende ich Google Colab und Whisper." />
  <meta name="author" content="Fatos Halimi" />
  <meta name="generator" content="Hugo 0.107.0">

  
  
      
      <link rel="stylesheet" href="https://fatoshalimi.com/css/styles.min.cf5878c2862fcc46f5a4a0088bf705a58278caf84242964cfb48294fcb97f247.css" integrity="sha256-z1h4woYvzEb1pKAIi/cFpYJ4yvhCQpZM+0gpT8uX8kc=">
  
  

  
  

  <meta property="og:title" content="Kostenloses Speech-To-Text AI Tool für Transkribieren mit Google Colab" />
<meta property="og:description" content="Whisper ist ein System zur automatischen Spracherkennung ASR) von OpenAI, dass in 680000 Stunden an überwachten Daten aus dem Internet trainiert und kostenlos in Github zur Verfügung gestellt.
Warum ist das so interessant? Nun hat man eine kostenlose Möglichkeit, Sprachaufnahmen direkt transkribieren und in andere Sprachen übersetzen zu lassen.
In dem Post schauen wir uns mal an, wie man beispielsweise die Aufnahme eines Interviews einfach und kostenlos transkribieren kann. Dazu verwende ich Google Colab und Whisper." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://fatoshalimi.com/blog/whisper_google_colab/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2022-09-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-09-28T00:00:00+00:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kostenloses Speech-To-Text AI Tool für Transkribieren mit Google Colab"/>
<meta name="twitter:description" content="Whisper ist ein System zur automatischen Spracherkennung ASR) von OpenAI, dass in 680000 Stunden an überwachten Daten aus dem Internet trainiert und kostenlos in Github zur Verfügung gestellt.
Warum ist das so interessant? Nun hat man eine kostenlose Möglichkeit, Sprachaufnahmen direkt transkribieren und in andere Sprachen übersetzen zu lassen.
In dem Post schauen wir uns mal an, wie man beispielsweise die Aufnahme eines Interviews einfach und kostenlos transkribieren kann. Dazu verwende ich Google Colab und Whisper."/>

  <meta itemprop="name" content="Kostenloses Speech-To-Text AI Tool für Transkribieren mit Google Colab">
<meta itemprop="description" content="Whisper ist ein System zur automatischen Spracherkennung ASR) von OpenAI, dass in 680000 Stunden an überwachten Daten aus dem Internet trainiert und kostenlos in Github zur Verfügung gestellt.
Warum ist das so interessant? Nun hat man eine kostenlose Möglichkeit, Sprachaufnahmen direkt transkribieren und in andere Sprachen übersetzen zu lassen.
In dem Post schauen wir uns mal an, wie man beispielsweise die Aufnahme eines Interviews einfach und kostenlos transkribieren kann. Dazu verwende ich Google Colab und Whisper."><meta itemprop="datePublished" content="2022-09-28T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-09-28T00:00:00+00:00" />
<meta itemprop="wordCount" content="536">
<meta itemprop="keywords" content="openai, whisper, interview transkribieren," />
</head>
<body class="dark:bg-gray-800 dark:text-white relative"><header class="container flex justify-around md:justify-between gap-4 flex-wrap p-6 mx-auto">
  <a href="https://fatoshalimi.com/" class="capitalize font-light text-4xl">
    
    FH
  </a>
  <ul class="flex items-center gap-4 lg:gap-6">

    
      <li><a href="/about">Über mich</a></li>
    

    
      <li><a href="https://fatoshalimi.com/blog/">Blog</a></li>
    

    
    <li class="grid place-items-center">
      <span class="open-search inline-block cursor-pointer">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
          <circle cx="10" cy="10" r="7" />
          <line x1="21" y1="21" x2="15" y2="15" />
        </svg>
      </span>
    </li>
    

    
    <li class="grid place-items-center">
      <span class="toggle-dark-mode inline-block cursor-pointer">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="12" cy="12" r="3" />
          <line x1="12" y1="5" x2="12" y2="5.01" />
          <line x1="17" y1="7" x2="17" y2="7.01" />
          <line x1="19" y1="12" x2="19" y2="12.01" />
          <line x1="17" y1="17" x2="17" y2="17.01" />
          <line x1="12" y1="19" x2="12" y2="19.01" />
          <line x1="7" y1="17" x2="7" y2="17.01" />
          <line x1="5" y1="12" x2="5" y2="12.01" />
          <line x1="7" y1="7" x2="7" y2="7.01" />
        </svg>
      </span>
    </li>
    
  </ul>
</header>
<main>

  
  <div class="relative max-w-5xl mx-auto px-4">
    <img src="/images/hitesh-choudhary-t1paibmtjim-unsplash.jpg" class="rounded-lg shadow-sm w-full object-contain" />
    <div class="absolute top-4 right-8 rounded shadow bg-white text-gray-900 dark:bg-gray-900 dark:text-white px-2 py-0.5">
      28.9.2022
    </div>
  </div>
  

  <article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4">

    <h1 class="text-2xl font-bold mb-2">Kostenloses Speech-To-Text AI Tool für Transkribieren mit Google Colab</h1>
    ⏱ Lesezeit: 2 Minuten.

    <p><a href="https://github.com/openai/whisper"  title="https://github.com/openai/whisper"  target="_blank" >Whisper</a> ist ein System zur automatischen Spracherkennung ASR) von <a href="https://openai.com/blog/whisper/"  title="https://openai.com/blog/whisper/"  target="_blank" >OpenAI</a>, dass in 680000 Stunden an überwachten Daten aus dem Internet trainiert und kostenlos in <a href="https://github.com/openai/whisper"  title="https://github.com/openai/whisper"  target="_blank" >Github</a> zur Verfügung gestellt.</p>
<p>Warum ist das so interessant? Nun hat man eine kostenlose Möglichkeit, Sprachaufnahmen direkt transkribieren und in andere Sprachen übersetzen zu lassen.</p>
<p>In dem Post schauen wir uns mal an, wie man beispielsweise die Aufnahme eines Interviews einfach und kostenlos transkribieren kann. Dazu verwende ich Google Colab und Whisper.</p>
<h1 id="was-ist-google-colab">Was ist Google Colab?</h1>
<p>Google Colab ist ein cloud-basierter Service um code zu entwickeln und auszuführen. Wie Google Docs nur fürs coding in Python.</p>
<p>Vor allem wenn man keinen leistungsstarken Laptop zu Hause hat, ist das eine super alternative.</p>
<h1 id="whisper-mit-google-colab-nutzen">Whisper mit Google Colab nutzen</h1>
<p>Als ersten Schritt brauchen wir ein Google Colab Notebook. Hierzu öffnest du diesen Link (<a href="https://colab.research.google.com/#create=true"  title="https://colab.research.google.com/#create=true"  target="_blank" >https://colab.research.google.com/#create=true</a>) und Google wird direkt ein neues Notebook erstellen.</p>
<p>Du kannst auch in deiner Google Drive mit einem Rechtsklick &gt; More &gt; Google Colaboratory ein neues Notebook anlegen.</p>
<p><picture>
    <source type="image/webp" srcset="/images/Google_Drive_Collab.webp">
    <source type="image/jpeg" srcset="/images/Google_Drive_Collab.png">
    <img src="/images/Google_Drive_Collab.png" alt=""  />
</picture>
</p>
<p>Das neue Notebook heißt dann Untitled.ipynb.</p>
<h2 id="gpu-aktivieren">GPU aktivieren</h2>
<p>Als Nächstes sagen wir Google, dass die GPU verwendet werden soll. Um das zu tun, gehen wir unter den Menüreiter Laufzeit &gt; Laufzeittyp ändern und wählen in dem Dropdown Hardwarebeschleuniger des Dialogs den Punkt GPU aus und speichern das Ganze.</p>
<p><picture>
    <source type="image/webp" srcset="/images/Google_Collab_Runtime.webp">
    <source type="image/jpeg" srcset="/images/Google_Collab_Runtime.png">
    <img src="/images/Google_Collab_Runtime.png" alt=""  />
</picture>
<picture>
    <source type="image/webp" srcset="/images/Google_Collab_GPU.webp">
    <source type="image/jpeg" srcset="/images/Google_Collab_GPU.png">
    <img src="/images/Google_Collab_GPU.png" alt=""  />
</picture>
</p>
<h2 id="whisper-installieren">Whisper installieren</h2>
<p>In dem neuen Notebook fügen wir als Code folgendes ein und führen dann mit dem Play-Button (oder mit CTRL + Enter) das ganze aus.</p>
<pre><code>!pip install git+https://github.com/openai/whisper.git 
!sudo apt update &amp;&amp; sudo apt install ffmpeg
</code></pre>
<p><picture>
    <source type="image/webp" srcset="/images/Google_Collab_whisper_install.webp">
    <source type="image/jpeg" srcset="/images/Google_Collab_whisper_install.png">
    <img src="/images/Google_Collab_whisper_install.png" alt=""  />
</picture>
</p>
<p>Somit installieren wir Whisper im Notebook. (Für weitere Installationshinweise gibt es unter <a href="https://github.com/openai/whisper#setup"  target="_blank" >https://github.com/openai/whisper#setup</a> &ldquo;<a href="https://github.com/openai/whisper#setup%22"  target="_blank" >https://github.com/openai/whisper#setup&#34;</a>))</p>
<p>Das Rufzeichen (!) an Anfang der Zeile fügen wir ein, weil wir ein shell script in Google Colabs ausführen und keinen Python-Code. Wenn du Whisper auf deinem Laptop nutzen willst, dann lass das Rufzeichen weg.</p>
<h2 id="audiodatei-hochladen">Audiodatei hochladen</h2>
<p>Damit wir die Audiodatei transkribieren können, müssen wir sie zuerst in Google Colab hochladen. Dazu könnt ihr sie mit Drag &amp; Drop in die Dateien-Ansicht von Google Collab oder mit dem Upload-Button hochladen.</p>
<p><picture>
    <source type="image/webp" srcset="/images/Google_Collab_File_Upload.webp">
    <source type="image/jpeg" srcset="/images/Google_Collab_File_Upload.png">
    <img src="/images/Google_Collab_File_Upload.png" alt=""  />
</picture>
</p>
<h2 id="transkribieren-mit-whisper">Transkribieren mit Whisper</h2>
<p>Nun fügen wir einen zweiten Code-Block in das Notebook mit dem +Code-Button ein und fügen folgende Zeilen in den Code-Block ein:</p>
<pre><code>!whisper &quot;test_interview.m4a&quot; --model medium --language German
</code></pre>
<p>Damit Magic passiert wieder auf den Play-Button (oder mit CTRL + Enter) ausführen.</p>
<p><picture>
    <source type="image/webp" srcset="/images/Google_Collab_whisper.webp">
    <source type="image/jpeg" srcset="/images/Google_Collab_whisper.png">
    <img src="/images/Google_Collab_whisper.png" alt=""  />
</picture>
</p>
<p>In diesem Beispiel habe ich das Model Medium und die Sprache Deutsch als Parameter mitgegeben.</p>
<p>Wenn die Transkription fertig ist, seht ihr in dem Datei-Explorer die fertig transkribierten Dateien.</p>
<h3 id="whisper-models">Whisper Models</h3>
<p>Whisper hat <a href="https://github.com/openai/whisper#available-models-and-languages"  title="https://github.com/openai/whisper#available-models-and-languages"  target="_blank" >verschiedene Modelle</a>, die ihr nützen könnt.</p>
<blockquote>
<p>Ein Model ist eine statistische Repräsentation der Speech-To-Text-Engine. Das Modell ist darauf trainiert, Sprache zu erkennen und sie für den Benutzer in Text umzuwandeln. Es gibt viele verschiedene Arten von Modellen, die jeweils für einen bestimmten Zweck entwickelt wurden.</p>
</blockquote>
<p>Als Standard wird das Small-Modell genommen. Es ist schneller, aber nicht so genau wie die Modelle darunter. Für eine genauere Verarbeitung nehmt ein größeres Modell.</p>
<h2 id="whisper-cmd-line-optionen">Whisper CMD-Line Optionen</h2>
<p>Du kannst direkt in Google Colab auch alle Optionen von Whisper anschauen:</p>
<pre><code>!whisper -h
</code></pre>
<h2 id="zusammenfassung">Zusammenfassung</h2>
<p>In dem Post haben wir uns die Basis-Steps für eine Transkription mit Google Colab und Whisper, dass wir in einem Google Colab über die Kommandozeile ausführen, angesehen.</p>
<p><strong>Happy Transcribing!</strong></p>
<h3 id="ressourcen">Ressourcen</h3>
<ul>
<li>OpenAI Blogpost &ldquo;Introducing Whisper&rdquo; <a href="https://openai.com/blog/whisper/"  target="_blank" >https://openai.com/blog/whisper/</a></li>
<li>Whisper Github-Repo: <a href="https://github.com/openai/whisper"  title="https://github.com/openai/whisper"  target="_blank" >https://github.com/openai/whisper</a></li>
<li>Google Colab: <a href="https://research.google.com/colaboratory/faq.html"  title="https://research.google.com/colaboratory/faq.html"  target="_blank" >https://research.google.com/colaboratory/faq.html</a></li>
</ul>

  </article><div class="bg-gray-50 dark:bg-gray-900">
  <div class="container px-6 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
    <div>
      <div class="text-2xl font-bold mb-2"></div>
      <p class="opacity-60"></p>
    </div>
  </div>
</div>
    </main><footer class="container p-6 mx-auto flex justify-between items-center">
  <span class="text-sm font-light">
    
    2022 © Fatos Halimi
    
  </span>
  <span onclick="window.scrollTo({top: 0, behavior: 'smooth'})" class="p-1 cursor-pointer">
    <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5"
      stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M18 15l-6 -6l-6 6h12" />
    </svg>
  </span>
  <span class="text-sm font-light">
    <a href="/impressum" target="_blank">Impressum & Datenschutz</a>
  </span>
</footer>

<div class="search-ui absolute top-0 left-0 w-full h-full bg-white dark:bg-gray-800 hidden">
  <div class="container max-w-3xl mx-auto p-12">
    <div class="relative">
      <div class="my-4 text-center text-2xl font-bold">Search</div>

      <span class="p-2 absolute right-0 top-0 cursor-pointer close-search">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <line x1="18" y1="6" x2="6" y2="18" />
          <line x1="6" y1="6" x2="18" y2="18" />
        </svg>
      </span>
    </div>

    <input type="search" class="py-2 px-3 w-full dark:text-black border dark:border-transparent"
      placeholder="Enter search query" />

    <div class="search-results text-lg font-medium my-4 hidden">Results</div>
    <ul class="search-list my-2">

    </ul>

    <div class="no-results text-center my-8 hidden">
      <div class="text-xl font-semibold mb-2">No results found</div>
      <p class="font-light text-sm">Try adjusting your search query</p>
    </div>
  </div>
</div>





<script src="https://fatoshalimi.com/js/scripts.min.js"></script>




<script>
  
  const darkmode = document.querySelector('.toggle-dark-mode');
  function toggleDarkMode() {
    if (document.documentElement.classList.contains('dark')) {
      document.documentElement.classList.remove('dark')
      localStorage.setItem('darkmode', 'light')
    } else {
      document.documentElement.classList.add('dark')
      localStorage.setItem('darkmode', 'dark')
    }
  }
  if (darkmode) {
    darkmode.addEventListener('click', toggleDarkMode);
  }

  const isDark = localStorage.getItem('darkmode');
  if (isDark && isDark === 'dark') {
    toggleDarkMode();
  }
</script>
</body>
</html>
